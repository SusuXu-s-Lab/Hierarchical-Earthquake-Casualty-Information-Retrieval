{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import itertools\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import pytz\n",
    "import time\n",
    "from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('China9_5news/death_tweets.csv')\n",
    "df_o = pd.read_excel('China9_5news/news_api_95_sichuan_content.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_list = df['time']\n",
    "death_list= df['death_score']\n",
    "location_list = df['city']\n",
    "country_list = df['country']\n",
    "target_country ='China'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = df['origin_id'].tolist()\n",
    "full_content=df_o['full content'].loc[id_list].tolist()\n",
    "url_l = df_o['url'].loc[id_list].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  import sys\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  import sys\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  import sys\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "from pandarallel import pandarallel\n",
    "from math import sin\n",
    "\n",
    "pandarallel.initialize()\n",
    "\n",
    "def clean_dict(x):\n",
    "    tmp_dict = yaml.load(x)\n",
    "    tmp_list = list(tmp_dict)\n",
    "    new_dict = {}\n",
    "    new_dict[tmp_list[0]] = tmp_dict[tmp_list[0]]\n",
    "    return new_dict\n",
    "\n",
    "death_list = death_list.parallel_apply(clean_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_loc(df, target_country, location_list,country_list):\n",
    "    location_binary = []\n",
    "    geoloc = []\n",
    "    from geopy.geocoders import Nominatim\n",
    "    geolocator = Nominatim(user_agent='myapplication')\n",
    "    for k, i in enumerate(location_list):\n",
    "        try:\n",
    "            if country_list[k] != target_country and isinstance(country_list[k], str):\n",
    "                if i and isinstance(i, str):\n",
    "                    tmp_location = geolocator.geocode(i+\" \"+country_list[k])\n",
    "                    if tmp_location and target_country in tmp_location[-2]:\n",
    "                        location_binary.append(True)\n",
    "                        geoloc.append(tmp_location[-1])\n",
    "                    else:\n",
    "                        location_binary.append(False)\n",
    "                        geoloc.append(None)\n",
    "                else:\n",
    "                    location_binary.append(False)\n",
    "                    geoloc.append(None)\n",
    "            elif country_list[k] == target_country:\n",
    "                if i and isinstance(i, str):\n",
    "                    tmp_location = geolocator.geocode(i+\" \"+target_country)\n",
    "                    location_binary.append(True)\n",
    "                    tmp_location = [-1] if not tmp_location else tmp_location\n",
    "#                     print(tmp_location)\n",
    "                    geoloc.append(tmp_location[-1])\n",
    "                else:\n",
    "                    tmp_location = geolocator.geocode(target_country)\n",
    "                    location_binary.append(True)\n",
    "                    geoloc.append(tmp_location[-1])\n",
    "            else:\n",
    "                if i and isinstance(i, str):\n",
    "                    tmp_location = geolocator.geocode(i+\" \"+target_country)\n",
    "                    location_binary.append(True)\n",
    "                    geoloc.append(tmp_location[-1])\n",
    "                else:\n",
    "                    location_binary.append(False)\n",
    "                    geoloc.append(None)\n",
    "        except:\n",
    "            time.sleep(80) \n",
    "            print(k)\n",
    "    return location_binary, geoloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df, time_list, death_list, location_binary):\n",
    "    num_list = []\n",
    "    prob_list = []\n",
    "    first_num_list = []\n",
    "    first_prob_list = []\n",
    "    for i, tmp_time in enumerate(time_list):\n",
    "        if location_binary[i]:\n",
    "            tmp_dict = death_list[i]\n",
    "            nan_key_list = []\n",
    "            for tmp_key_0 in tmp_dict.keys():\n",
    "                if tmp_key_0 == 'none':\n",
    "                    tmp_key_0 = '-1'\n",
    "                if not tmp_key_0.isnumeric() and not tmp_key_0 == '-1':\n",
    "                    nan_key_list.append(tmp_key_0)\n",
    "            for tmp_key_0 in nan_key_list:\n",
    "                del tmp_dict[tmp_key_0]\n",
    "            tmp_len = len(tmp_dict.keys())\n",
    "            tmp_list_num = []\n",
    "            tmp_list_prob = []\n",
    "            if tmp_len <= 1:\n",
    "                for tmp_token, tmp_dict_2 in tmp_dict.items():\n",
    "                    if tmp_token == 'none':\n",
    "                        tmp_token = '-1'\n",
    "                    if tmp_token.isnumeric() or tmp_token == '-1':\n",
    "                        for tmp_token_2, tmp_prob in tmp_dict_2.items():\n",
    "                            if tmp_token_2 == 'none':\n",
    "                                tmp_token_2 = '-1'\n",
    "                            if tmp_token_2.isnumeric() or tmp_token_2 == '-1':\n",
    "                                tmp_list_num.append(int(tmp_token_2))\n",
    "                                tmp_list_prob.append(tmp_prob) \n",
    "\n",
    "            else:\n",
    "                count_up = min(2, 5 - tmp_len)\n",
    "                tmp_list_n = []\n",
    "                tmp_list_p = []\n",
    "                for tmp_token, tmp_dict_2 in tmp_dict.items():\n",
    "                    tmp_list_num_2 = []\n",
    "                    tmp_list_prob_2 = []\n",
    "                    if tmp_token == 'none':\n",
    "                        tmp_token = '-1'\n",
    "                    if tmp_token.isnumeric() or tmp_token == '-1':\n",
    "                        count = 0\n",
    "                        for tmp_token_2, tmp_prob in tmp_dict_2.items():\n",
    "                            if count < count_up:\n",
    "                                if tmp_token_2 == 'none' or tmp_token_2 == '-1':\n",
    "                                    tmp_token_2 = '-1'\n",
    "                                if tmp_token_2.isnumeric():\n",
    "                                    tmp_list_num_2.append(tmp_token_2)\n",
    "\n",
    "                                    tmp_list_prob_2.append(tmp_prob) \n",
    "                                    count += 1\n",
    "\n",
    "                    tmp_list_n.append(tmp_list_num_2)\n",
    "                    tmp_list_p.append(tmp_list_prob_2)\n",
    "\n",
    "                count = 0\n",
    "                tmp_list_n = list(itertools.product(*tmp_list_n))\n",
    "                tmp_list_p = list(itertools.product(*tmp_list_p))\n",
    "                for i, item_n in enumerate(tmp_list_n):\n",
    "                    tmp_str = \"\".join(n for n in item_n if n )\n",
    "                    tmp_list_num.append(int(tmp_str) if tmp_str else -1)\n",
    "                    tmp_list_prob.append(np.prod(tmp_list_p[i]))\n",
    "\n",
    "            new_tmp_list_p = [x for x,y in sorted(zip(tmp_list_prob,tmp_list_num),key=lambda x: x[0],reverse=True)]\n",
    "\n",
    "            new_tmp_list_n = [y for x,y in sorted(zip(tmp_list_prob, tmp_list_num),key=lambda x: x[0],reverse=True)]\n",
    "            \n",
    "            tmp_l = min(len(new_tmp_list_n), 5)\n",
    "            num_list.append(new_tmp_list_n[:tmp_l])\n",
    "            prob_list.append(new_tmp_list_p[:tmp_l])\n",
    "            if len(new_tmp_list_n) > 0:\n",
    "                first_num_list.append(new_tmp_list_n[0])\n",
    "            else:\n",
    "                first_num_list.append(None)\n",
    "            if len(new_tmp_list_p) > 0:\n",
    "                first_prob_list.append(new_tmp_list_p[0])\n",
    "            else:\n",
    "                first_prob_list.append(None)\n",
    "        else:\n",
    "                num_list.append(None)\n",
    "                prob_list.append(None)\n",
    "                first_num_list.append(None)\n",
    "                first_prob_list.append(None)\n",
    "    return num_list, prob_list, first_num_list, first_prob_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_results = process_loc(df, target_country, location_list,country_list)\n",
    "location_binary, geoloc = geo_results\n",
    "location_binary = [True]*len(location_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the wiki request sometimes will break when reaching the upper limit, I setup resume after 80 seconds, but you will need to drop the missed rows to match.\n",
    "# drop_list = [70]\n",
    "# drop_list = [5213,5927]#731\n",
    "# drop_list = [473,970,1153]#729\n",
    "# drop_list = [2806,10267]#728\n",
    "# new_time_list = time_list.drop(time_list.index[drop_list])\n",
    "# new_death_list = death_list.drop(death_list.index[drop_list])\n",
    "# new_time_list.reset_index(drop=True, inplace=True)\n",
    "# new_death_list.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_time_list = time_list\n",
    "new_death_list = death_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = process_data(df, new_time_list, new_death_list,location_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_list, prob_list, first_num_list, first_prob_list = results\n",
    "num_s = pd.Series(num_list)\n",
    "prob_s = pd.Series(prob_list)\n",
    "first_num_s = pd.Series(first_num_list)\n",
    "first_prob_s = pd.Series(first_prob_list)\n",
    "geo_s = pd.Series(geoloc)\n",
    "loc_b_s = pd.Series(location_binary)\n",
    "df['variables_death'] = num_s\n",
    "df['probs_death'] = prob_s\n",
    "df['variables_top1_death'] = first_num_s\n",
    "df['probs_top1_death'] = first_prob_s\n",
    "df['geo'] = geo_s\n",
    "df['location_correct?'] = loc_b_s\n",
    "df['deaths'] = first_num_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  import sys\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  import sys\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  import sys\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "death_list= df['injury_score']\n",
    "death_list = death_list.parallel_apply(clean_dict)\n",
    "# new_death_list = death_list.drop(death_list.index[drop_list])\n",
    "# new_death_list.reset_index(drop=True, inplace=True)\n",
    "new_death_list = death_list\n",
    "results_2 = process_data(df, new_time_list, new_death_list,location_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_list, prob_list, first_num_list, first_prob_list = results_2\n",
    "num_s = pd.Series(num_list)\n",
    "prob_s = pd.Series(prob_list)\n",
    "first_num_s = pd.Series(first_num_list)\n",
    "first_prob_s = pd.Series(first_prob_list)\n",
    "geo_s = pd.Series(geoloc)\n",
    "loc_b_s = pd.Series(location_binary)\n",
    "df['variables_injury'] = num_s\n",
    "df['probs_injury'] = prob_s\n",
    "df['variables_top1_injury'] = first_num_s\n",
    "df['probs_top1_injury'] = first_prob_s\n",
    "df['injurues'] = first_num_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_time = pd.to_datetime(df['time'],format=\"%Y-%m-%dT%H:%M:%SZ\", errors='coerce', utc=False)\n",
    "utc_time = new_time.dt.tz_localize('US/Eastern').dt.tz_convert('UTC')\n",
    "df['UTC_time'] = utc_time.dt.tz_localize(None)\n",
    "df['full_content'] = pd.Series(full_content)\n",
    "df['url'] = pd.Series(url_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('China9_5news/death_tweets_processed.csv')\n",
    "pd.DataFrame.to_excel(df,'China9_5news/death_tweets_processed.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
